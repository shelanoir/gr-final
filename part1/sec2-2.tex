\documentclass[part1.tex]{subfiles}
\begin{document}

\section{Resolution in HA-based logic}
\subsection{Overview of inference in LSHA-based propositional logic}
\paragraph{Logical consequence:} A is a logical consequence of B if every interpretation 
satisfying A also satisties B. Notated: A \(\models\) B.\\
\paragraph{Inference rule:} An inference rule of the form\\
\begin{align*}
	\frac{F_1, F_2,\cdots,F_n}{G}
\end{align*}
means that with a given set of formula \(F_1,F_2,\cdots,F_n\), we can deduce a 
new formula \(G\). An inference rule is sound if it satisfies logical consequence.\\
To be useful, aside from being sound, an inference rule also should be complete, meaning\\
it deduce every formula that is a logical consequence of the original set of formulae.\\
\paragraph{Confidence value:}
In fuzzy logic, inference rules are only approxiamtely true, so formulae derived by them must be
taken with a {\em confidence value}. A formula F with \(\alpha\) confidence is denoted by \(F_\alpha\).\\

Clauses in the initial knowledge base are given with some confidence value. The confidence of formulae
derived from them will be determined by their given confidence value. Obviously the derived formula's 
confidence must not be larger than its original formulae's.

\subsection{Resolution rule}
\paragraph{Resolution rule:} Given \(A^a \vee B^{b1}\) and \(C^c \vee B^{b2}\). If \(b1 \vee b2 
\ge W\) and \(b1 \wedge b2 < W\):
\begin{align*}
	&\frac{(A^a \vee B^{b1}, {\alpha 1}), (C^c \vee B^{b2}, {\alpha 2})} {(A^a \vee C^c, {\alpha 3})}\\
	&\text{Where: }
	\begin{cases}
	\alpha 1, \alpha 2, \alpha 3 \text{ are the confidence
        values of formulae.}\\
	A, B, C \text{ are propositional symbols}\\
	a, b1, b2, c \text{ are linguistic value}
	\end{cases}
	\\
	&\text{If } (A^a \vee B^{b1}, {\alpha 1}) \wedge (C^c \vee B^{b2}, {\alpha 2}) \text{ is 
	unsatisfiable, the result of resolution is the null clause.} 
\end{align*}
\paragraph{} Since the resolution rule only applies to
disjuntions of literals, to be able to reasoning using
resolution, we first have to transform the original formulae into
a conjuntion of disjunction of literals, or a CNF.  The procedure
to achieve this was discussed in the previous section.\\

\subsection{Resolution algorithm:} 
 To prove that a goal clause G entails the knowledge base KB, e.g \(KB \models G\), we can instead prove
that \(KB \cup \neg G\) is unsatisfiable.\\\\
\indent {\em Input: S = \(KB \cup \neg G\)}\\
\indent {\em Output: S is unsatisfiable or not}\\\\
\indent{\bfseries BEGIN}\\
\indent If (S contains NULL) thenn return true;\\
\indent Foreach (clause C1 in S)\\
\indent \indent Foreach (clause C2 in S)\\
\indent \indent \indent If (C1 and C2 can be resolved) C3 := resolve(C1,C2);\\
\indent \indent \indent If (C3 = NULL) return Confidence(C3);\\
\indent \indent \indent Else Reduce(C3)\\
\indent \indent \indent \indent (Add C3 to S);\\
\indent If (S does not contain NULL) then return Nothing;\\
\indent{\bfseries END}
\paragraph{} Where Confidence(C3) is the confidence value of
C3, and Reduce(C3) is C3 after it was reduced by elimination of
redundant literals. The following subsections will further
elaborate on them.

\subsection{Reducing disjunction of literals}
\paragraph{} Unlike classical propositional logic,
literals in this fuzzy propositional logic can say the same thing
(i.e. have the same symbol) but have different truth value
annotated to them which are significant in interpretation, and
eliminating arbitrary literals among them may change the clause's
semantics. Because of this, we have to consider a way to safely
reduce disjunction-of-literals without altering its semantics.
\paragraph{Theorem:} Given a symbol A, for any interpretation
I:\{A = c\}, we have:
\[
   \begin{cases}
           T_{I:\{A = c\}}(A^{a} \vee A^{b}) &=  T_{I:\{A =
           c\}}(A^{a \vee b})  \text{ if } c > W \\

           T_{I:\{A = c\}}(A^{a} \vee A^{b}) &=  T_{I:\{A =
           c\}}(A^{a \wedge b}) \text{ if } c < W
   \end{cases}
\]

\subsubsection{Proof:}
{\bfseries Case 1: a, b > W, c > W}
\begin{align*}
        T_{I:\{A = c\}}(A^{a} \vee A^{b}) &= T_{I:\{A =
                 c\}}(A^{a}) \vee T_{I:\{A = c\}}(A^{b}) \\
                 &= (a \wedge c) \vee (b \wedge c) \\
                 &= (a \vee b) \wedge c \\
                 &= T_{I:\{A = c\}}(A^{a \vee b}) \\
\end{align*}

{\bfseries Case 2: a, b > W, c < W}
\begin{align*}
        T_{I:\{A = c\}}(A^{a} \vee A^{b}) 
                 &= T_{I:\{A = c\}}(A^{a}) \vee T_{I:\{A = c\}}(A^{b}) \\
                 &= (\neg{a} \vee c) \vee (\neg{b} \vee c) \\
                 &= (\neg{a} \vee \neg{b}) \vee c \\
                 &= \neg{(a \wedge b)} \vee c \\
                 &= T_{I:\{A = c\}}(A^{a \wedge b}) \\
\end{align*}

{\bfseries Case 3: a, b < W, c > W}
\begin{align*}
        T_{I:\{A = c\}}(A^{a} \vee A^{b}) 
                 &= T_{I:\{A = c\}}(A^{a}) \vee T_{I:\{A = c\}}(A^{b}) \\
                 &= (a \vee \neg{c}) \vee (b \vee \neg{c}) \\
                 &= (a \vee b) \vee \neg{c} \\
                 &= T_{I:\{A = c\}}(A^{a \vee b}) \\
\end{align*}

{\bfseries Case 4: a, b < W, c < W}
\begin{align*}
        T_{I:\{A = c\}}(A^{a} \vee A^{b}) 
                 &= T_{I:\{A = c\}}(A^{a}) \vee T_{I:\{A = c\}}(A^{b}) \\
                 &= \neg{(a \vee c)} \vee \neg{(b \vee c)} \\
                 &= (\neg{a} \wedge \neg{c}) \vee (\neg{b} \wedge \neg{c})\\
                 &= (\neg{a} \vee \neg{b}) \wedge \neg{c}\\
                 &= \neg{(a \wedge b)} \wedge \neg{c}\\
                 &= \neg{((a \wedge b) \vee c)}\\
                 &= T_{I:\{A = c\}}(A^{a \wedge b}) \\
\end{align*}

{\bfseries Case 5: a > W, b < W, c > W}
\begin{align*}
        T_{I:\{A = c\}}(A^{a} \vee A^{b}) 
                 &= T_{I:\{A = c\}}(A^{a}) \vee T_{I:\{A = c\}}(A^{b}) \\
                 &= (a \wedge c) \vee (b \vee \neg{c}) \\
                 &= (a \vee b \vee \neg{c}) \wedge (b \vee \neg{c} \vee c)\\
                 &= (a \vee b \vee \neg{c}) \wedge (b \vee c)\\
                 &= (a \wedge b) \vee (a \wedge c) \vee (b \wedge
                        b) \vee (b \wedge c) \vee (b \wedge \neg{c}) \vee (c \wedge
                        \neg{c})\\            
                 &= (a \wedge c) \text{\: (for $a \wedge c = max(a \wedge
                        b, a \wedge c, b \wedge
                        b = b, b \wedge c, b \wedge \neg{c}, c \wedge
                        \neg{c} = \neg{c})$)}\\        
                 &= (a \vee b) \wedge c \text{\: (for $a
                        \vee b = max(a,b) = a$)} \\
                 &= T_{I:\{A = c\}}(A^{a \vee b}) \\
\end{align*}

{\bfseries Case 6: a > W, b < W, c < W}
\begin{align*}
        T_{I:\{A = c\}}(A^{a} \vee A^{b}) 
                 &= T_{I:\{A = c\}}(A^{a}) \vee T_{I:\{A = c\}}(A^{b}) \\
                 &= (\neg{a} \vee c) \vee \neg{(b \vee c)} \\
                 &=  (\neg{a} \vee c) \vee (\neg{b} \wedge \neg{c})\\
                 &= (\neg{a} \vee \neg{b} \vee c) \wedge (\neg{a} \vee c \vee \neg{c}\\
                 &= (\neg{a} \vee \neg{b} \vee c) \wedge (\neg{a} \vee \neg{c}\\
                 &= \neg{a} \vee ((\neg{b} \vee c) \wedge \neg{c}))\\
                 &= \neg{a} \vee (\neg{b} \wedge \neg{c})\\
                 &= \neg{b} \wedge \neg{c} \text{
        \: (for $(\neg{b} \wedge \neg{c}) = max(\neg{a}, \neg{b} \wedge \neg{c})$)}\\
                 &= \neg{(b \vee c)}\\
                 &= \neg{(b \wedge a) \vee c} \text{\: (for $b = min(a,b)$)}\\
                 &= \neg{((a \wedge b) \vee c)}\\
                 &= T_{I:\{A = c\}}(A^{a \wedge b}) \\
\end{align*}
\paragraph{} From {\bfseries case 1 -> case 6}\\
$\implies$ \(
   \begin{cases}
           T_{I:\{A = c\}}(A^{a} \vee A^{b}) &=  T_{I:\{A =
           c\}}(A^{a \vee b})  \text{ if } c > W \\

           T_{I:\{A = c\}}(A^{a} \vee A^{b}) &=  T_{I:\{A =
           c\}}(A^{a \wedge b}) \text{ if } c < W
   \end{cases} 
   \) (q.e.d).

\subsection{Confidence value in HA-based logic}
By the definition of resolution, we can see for the two clauses to be able to resolve, \(b_1\) and 
\(b_2\) must be on different side w.r.t. W, and the bigger their difference, the more reliable the 
inference. We consider 4 methods to determine the confidence value of an inference step.\\
\subsubsection{Confidence using min}
Obviously, the inference step is more certain as the smaller value of \(b_1\) and \(b_2\) is closer to 
0. In particular, the confidence value of the inferred clause is:
\begin{align*}
	{\alpha}_3 = \wedge({\alpha}_1, {\alpha}_2, \neg(b_1 \wedge b_2))
\end{align*}
\subsubsection{Confidence using max}
Conversely, the inference step is more certain as the bigger value of \(b_1\) and \(b_2\) is closer to 
1. In particular, the confidence value of the inferred clause is:
\begin{align*}
	{\alpha}_3 = \wedge({\alpha}_1, {\alpha}_2, (b_1 \vee b_2))
\end{align*}
\subsubsection{Confidence using max and min}
Each of the previous method can only express part of the confidence of the resolution. We can combine
them to get a confidence value that represent both the ``false'' and ``true'' values \(b_1\) and 
\(b_2\). In particular, the confidence value of the inferred clause is:
\begin{align*}
	{\alpha}_3 = \wedge({\alpha}_1, {\alpha}_2, (b_1 \vee b_2), \neg(b_1 \wedge b_2))
\end{align*}
\subsubsection{Confidence using the semantic difference of two value}
In the previous methods, we only consider how right (or wrong) \(b_1\) and \(b_2\) are to determine
the confidence of our inference. To better evaluate the confidence value, we must be able to express
the difference between \(b_1\) and \(b_2\), as the more different they are the more certain our 
inference is and vice versa.\\
\\
Assuming we have the indexing function \(h: X \to N\) mapping a linguistic value to the number of 
elements in X that is no larger than it. Notice that the number of elements in X must be finite then, 
which means the maximum length of hedge strings in HA must also be finite.\\ 
\\
The difference between to linguistic value x1 and x2 is then defined by:
\begin{align*}
	dif(x1,x2) = x, \text{ where: } h(x) = h(x1) - h(x2)\\
\end{align*}
The confidence value is:
\begin{align*}
	\alpha_3 = \wedge(\alpha_1,\alpha_2,dif(b_1,b_2))
\end{align*}
\\
This definition of confidence value is the most accurate out of the four. It clearly states the relation
between two ``opposing'' value \(b_1\) and \(b_2\). The only downside is that we have to limit the 
length of hedge strings to be able to calculate the difference. It is shown that there is an 
isomorphism between the set X of a LHA {X,G,H,\(\le\)} and the interval [0..1]. If we had a computable
method to map each value in X to a value in [0..1], we could calculate the difference even more 
accurately, and more importantly, without limitting the maximum length of hedge strings.\\

\paragraph{}From here on out, we would implicitly mean the third method
(combining max and min) when talking about confidence calculating
method, unless it was said otherwise.


\subsection{Proof of Soundness}
Proof of soundness means that we have to prove: \(KB {\vdash}_{resolution} G \implies KB \models G\). To
put in English: the if resolution method can derive G from KB then G is satisfiable in every model of 
KB. We can instead prove that \(S = KB \cup \neg G\) is unsatisfiable if doing resolution on
S derives the null clause.\\

\paragraph{Lemma 1:} Let \(A^a \vee B^{b1}, C^c \vee B^{b2}\) be the input clauses of a resolution, and 
\(A^a \vee C^c\) is the result. If \(A^a \vee C^c\) is false under an interpretation I, then 
\(A^a \vee B^{b1}, C^c \vee B^{b2}\) is also false under I.\\
\indent Proof:
\begin{align*}
	&T((A^a \vee B^{b1}) \wedge (C^c \vee B^{b2}))\\
	&= T(A^a \wedge C^c) \vee T(A^a \wedge B^{b2}) \vee T(C^c \wedge B^{b1}) \vee T(B^{b1} \wedge 
	B^{b2})\\
	&\text{We have:}\\
	&W > T(A^a \vee C^c) \ge T(A^a \wedge C^c)\\
	&W > T(A^a \vee C^c) \ge T(A^a) \ge T(A^a \wedge B^{b2})\\
	&W > T(A^a \vee C^c) \ge T(C^c) \ge T(C^c \wedge B^{b1})\\
	&W > T(B^{b1} \wedge B^{b2}) \text{  (since b1 and b2 are in different sides w.r.t. W)}\\
	&\implies T((A^a \vee B^{b1}) \wedge (C^c \vee B^{b2})) < W \text{ (q.e.d)}
\end{align*}

\paragraph{Theorem:} If S \(\vdash_{resolution}\) NULL \(\implies\) S is unsatisfiable.\\
\indent Proof: Let S1 be the set of all clauses after doing resolution on S. We have \(null \in S1\), so
S1 is unsatisfiable. According to Lemma 1, S must also be unsatisfiable (q.e.d).\\
\subsection{Proof of Completeness}
Proof of completeness means that we have to prove: \(KB \models G \implies KB {\vdash}_{resolution} G\). To put in English: the resolution method can derive G from KB if G is satisfiable in every model of 
KB. We can instead prove that if \(S = KB \cup \neg G\) is unsatisfiable,  doing resolution on
S derives the null clause.\\

\subsubsection{Proof by model constructing}
\paragraph{Well-founded ordering:} a binary relation \(<\) on a set S is a well-founded ordering if:
\begin{itemize}
	\item non-reflexive
	\item transitive
	\item there exists a least element in (S, \(<\))
\end{itemize}
\paragraph{Transfinite induction:} if \(B \subset A, A\) is well-founded ordered by \(<\), for every a in A:\\
\(({\forall c \in A, c < a} \subset B \implies a \in B) \implies A = B\)\\
\(\to\) if for some property P, \(\forall y < x, P(y) = true \implies P(x) = true\), then P is true for 
every x in A.
We can see transfinite induction is a generalization of the more familiar induction on the set of integer.\\
\paragraph{Proof of completeness}:\\
{\bfseries Lemma 2: If S is the transitive closure of resolution and S doesn't contain NULL, then S is
satisfiable\\}
Proof: We will prove this by constructing a model of S. Let I be an arbitrary interpretation of S, J be
our being constructed model of S.\\
Let False(I) be the set of false clauses of S under I, FalseL(I) the set of false literals of S under I.
Similarly we have True(I) and TrueL(I).
Let Y be the minimal cover of False(I), meaning that for all C in False(I), it contains at least one 
literal in Y, and Y is the smallest possible set satisfies that. It is trivial to see that for each 
literal in C there exists a clause C in False(I) that L is the only literal that both C and Y contain
(1).\\

Let's construct J such that:
\begin{itemize}
	\item \(T_J(L) \ge W \) if \(L \in Y\)
	\item \(T_J(L) \ge W \) if \(L \in TrueL(I)\) and there is no complement of L in Y
	\item \(T_J(L) < W\) otherwise
\end{itemize}

Now let \(<_1\) be a binary relation on S such that:\\
\(R <_1 C \iff R\) is a resolution of \(C\) and \(C_0 \in False(I)\). \(<_1\) is a 
well-founded ordering.\\
\\
We have to prove that: for all C in S, \(\forall (R <_1 C, T_J(R) \ge W) \to T_J(C) \ge W\).\\
If C is in False(I) then we always have \(T_J(C) \ge W\). If C is in True(I), assume that 
\(T_J(C) < W, \to \forall\) literals in C \(\in FalseL(J)\). C in True(I) so there exists a
literal \(L \in TrueL(I) \cap FalseL(J), \to \exists \text{the complement} L_1\) of L. \(L_1 \in
FalseL(I) \cap TrueL(J) \equiv Y\). From (1), we have C' being the clause in False(I) that has only 
\(L_1\) as a literal from Y \(\to L_1\) is the only true literal in C'. Doing resolution on C and C',
we have \(R1 = (C - L) \cup (C' - L1) \to R1\) contains only false literal under J. We also have R1
\(<_1\) C, so R1 must be true under J. So the assumption that \(T_J(C) < W\) is false, and \(T_J(C) 
\ge W\). Applying transfinite induction, we have \(T_J(C) \ge W\) in S. (q.e.d)\\

Applying Lemma 2, we have that for an unsatisfiable set of clauses S, resolution will always derive the
null clause. So the resolution inference rule is complete. (q.e.d)\\\\
\subsubsection{Proof by semantic tree}
\paragraph{Semantic tree: } Given a set of clauses S containing n symbols \(A_1..A_n\). The semantic
tree if S is a n-depth complete binary tree, each level corresponding to a literal. Children branches
of a node is labeled \(A_i < W\) and \(A_i > W\). 
\(\implies\) The semantic tree represents every interpretation possible for S, with each branch from
root to a leaf corresponding to an interpretation. The trivial case of \(A_i = W\) is not considered.\\\\
\begin{figure}[h]
        \centering
\xytree{
	%&&\xynode[-2,-1,0,1,2]{S}\\
        &&&&\xynode{} \xyconnect{1,-2}"_{\(A_{1} < W\)}" \xyconnect{1,2}"^{\(A_{1} > W\)}"\\
        &&\xynode{} \xyconnect{1,-1}"_{\(A_{2} < W\)}" \xyconnect{1,1}"^{\(A_{2} > W\)}" 
	&&&& \xynode{} \xyconnect{1,-1}"_{\(A_{2} < W\)}" \xyconnect{1,1}"^{\(A_{2} > W\)}"\\ 
         &\xynode{} && \xynode{} && \xynode{} && \xynode{}\\ 
}
\caption{Semantic tree example}
\end{figure}
\\
\paragraph{Failure node: } A node is a failure node if there exists an interpretation corresponding to
the path from root to that node that S is false under that interpretation, and S is not false at 
any ancestor of that node.\\
\paragraph{Failure tree: } If every path from root to leaf contains a failure node, cut off the children
of those failure nodes, we get a failure tree.\\
\paragraph{Inference node: } A node in failure tree where both of its children are failure node.\\
\(\implies\) S is unsatisfiable if and only if it has a failure tree (1)\\
\(\implies\) There exists at least one inference node in a failure tree (2)\\
If a failure tree contains only one node, then it is false for the null interpretation.
Only null clause is false for null interpretation, so if S's failure tree contains only one node, S
must contain the null clause (3)\\
%
\paragraph{Proof of completeness: } Now we are going to prove that if \(S = KB \cup \neg G\) is 
unsatisfiable, resolution will derive the null clause from S. \\
According to (1), S has a failure tree. According to 2, S has at least one inference node.\\
\begin{figure}[H]
\xytree{
	%&&\xynode[-2,-1,0,1,2]{S}\\
        &&&&&&&\xynode{i} \xyconnect{1,-2}"_{\(A_{m} < W\)}" \xyconnect{1,2}"^{\(A_{m} > W\)}"\\
        &&&&&\xynode{j} &&&& \xynode{k} \\
}
\caption{an inference node i}
\end{figure}
Let i be an inference node with j and k are its children. Because S is false at j where \(A_m < W\),
there must be a false clause C1 in S in the form \(...\vee {A_{m}}^{a1}, a1 > W\) and every other 
literal in C1 must be false at i. Similarly, there exist a C2 in S that false at k, and it has the form 
\(\cdots\vee {A_{m}}^{a2}, a2 > W\), every other literal in C2 must also be false at i.
Doing resolution on C1 and C2, we get C which contains every false literal in C1, C2 but \(A_{m}\).
Then we have \(S \vee C\)'s failure tree is S's failure tree cutting off j and k.\\
Repeat this process until there is only one node left. Now we have the transitive closure of resolution
on S, and this closure contain the null clause according to (3) (q.e.d).
\\
\subsection{$\alpha-Resolution$ stragety in HA-based logic}
\paragraph{} Since different proofs of the same clause may have different reliabilities, 
it is natural to study how to design a resolution procedure with the best reliability. 
Below we present such a procedure.
\paragraph{} 
We say that a set of clauses S is saturated iff for every fuzzy linguistic reso-
lution inference with premises in S, the conclusion of this inference is a variant
with smaller or equal reliability of some clauses in S. That is for every fuzzy
linguistic resolution inference:\\
\[\indent \frac{(C1, \alpha1), (C2, \alpha2)}{(C,\alpha)}
\]
\indent there is some clause \((C',\alpha') \in \) S such that \(\alpha' \ge \alpha\).
\paragraph{} 
We introduce a resolution strategy, called $\alpha$-strategy, which guarantees that
the resolution proof of each clause has the maximal reliability. An $\alpha$-strategy
derivation is a sequence of the form $S_{0},\ldots,S_{i},\ldots$, where
\begin{itemize} 
	\item each Si is a set of clauses with reliability, and
	\item  \(S_{i+1}\) is obtained by adding the conclusion of a fuzzy 
	linguistic resolution inference with premises with maximal reliabilities 
	from Si , that is \(S_{i+1} = Si \cup \{(C, \alpha)\}\), 
	where (C, $\alpha$) is the conclusion of the fuzzy linguistic resolution inference  
	\(\frac{(C1 , \alpha1), (C2 , \alpha2)}	{(C, \alpha)}\)
	\\\\	
	$(C1 , \alpha1 ), (C2 , \alpha2 ) \in S_{i}$ and there are not any clauses 
	with reliability $(C1 , \alpha_{1}' ), (C2 , \alpha_{2}' ) \in S_{i}$ such that
			$\alpha_{1}' > \alpha_{1}$ and $\alpha_{2}' > \alpha_{2}$ , or

		\item $S_{i+1}$ is obtained by removing a variant with smaller 
			reliability, that is $S_{i+1} = S_{i} \setminus \{(C, \alpha)\}$ 
			where $(C, \alpha) \in S_{i}$ and there is some
			$(C, \alpha') \in S_{i}$ such that $\alpha < \alpha'$.
\end{itemize}
\paragraph{} 
{\bfseries The limit of a derivation $S_{0},\ldots,S_{i},\ldots$:} 
$S_{\infty} = \underset{i \ge 0}{\cup} \underset{j \ge i}{\cap}S_{j}$
\\

\paragraph{} The proof of completeness for $\alpha-Resolution$ is a direct
consequence of the proof of completeness for normal resolution
using proof tree, since $\alpha-Resolution$ still has the 
same proof tree with the same nodes like its normal resolution
equivalence.

\paragraph{} The following result establishes the soundness of
the $\alpha-Resolution$ procedure.
\paragraph{Theorem 2}
Let $S_{0},\ldots,S_{i},\ldots$ be a fuzzy linguistic resolution $\alpha$-strategy derivation.
Sn contains the empty clause iff S0 is unsatisfiable (for some n = 0, 1,\ldots).
\paragraph{Lemma 2} 
Consider the following resolution inferences:
\[ \frac{(A^{a} \vee B^{b1}, \alpha), (B^{b2} \vee C^{c}, \beta)} {(A^{a} \vee C^{c}, \gamma)}\]
\[\frac{(A^{a} \vee B^{b1}, \alpha), (B^{b2} \vee C^{c}, \beta')} {(A^{a} \vee C^{c}, \gamma')}\]
Then, $\beta' > \beta$ implies $\gamma' \ge \gamma$

\paragraph{Lemma 3} Let $S_{0},\ldots,S_{i},\ldots$ be a fuzzy linguistic 
resolution $\alpha$-strategy derivation, and $S_{\infty}$ be the the limit of the derivation.
Then $S_{\infty}$ is saturated.

\paragraph{Theorem 3} Let $S_{0},\ldots,S_{i},\ldots$ be a fuzzy linguistic 
resolution $\alpha$-strategy derivation, and $S_{\infty}$ be the the limit of the derivation. 
Then for each clause $(C, \alpha)$ in $S_{\infty}$ , there is not any other resolution proof 
of the clause $(C, \alpha')$ from $S_{0}$ such that $\alpha' > \alpha$.
\end{document}
