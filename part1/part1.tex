\documentclass[../gr-final.tex]{subfiles}
\begin{document}
\part{Problem definition, Background theory and Implementation Approaches}
\chapter{Introduction}
\paragraph{} This chapter presents the motivations for 
\section{Hedge Algebra and Fuzzy logic}
\subsection{Traditional fuzzy logic}
\paragraph{} More than just a tool for communication, natural
languages are also the framework we use to express our
understanding of the world, to think about it, to analyse it, reason about it
and to make decisions. By nature, natural languages are
vague, imprecise and qualitative. To model the thinking process
of our own using formal languages, we need a tool to capture this
``fuzziness'' property. Motivated by this, fuzzy sets theory and
fuzzy logic was founded in 1965 by Zadeh and since then has been
extensively developed. 
\paragraph{} Fuzzy logic deals with approximate reasoning, in
constrast to classical logic. The truth value domain is not the
binary set \{True, False\} or \{1,0\}, but rather between completely
false and completely true, i.e the whole {\em interval} [0,1], or
as we shall see, a range of {\em linguistic truth values}, whose
values are actually labels for the underlying numerical values.
\paragraph{} From fuzzy sets theory and fuzzy logic,
several new possibilities were unfolded for both research
directions and real world applications in various areas,
especially artificial intelligence. While fuzzy sets theory is
undoubtably a very fruitful approach for handling vagueness
and uncertainty in human's reasoning, however just that alone is
not enough to model linguistic reasoning, in other words, how we
human actually {\em think}. To achieve that end, we need some
extension to plain fuzzy sets theory, namely {\em linguistic
variables} and {\em linguistic hedges}
\subsection{Linguistic variables and linguistic hedges}
\paragraph{} In his exploration for a framework to model human's
reasoning process, Zadeh introduced the concept of {\em linguistic
variable}$^{\text {citation needed}}$ in 1975. A linguistic
variable is a variable whose values are linguistic, e.g tall,
short, very-tall, approximately-tall, possibly-not-very-short...
rather than numerical. These linguistic values are mapped to a
numerical value in the interval [0..1] by a {\em compatibility
function}. A typical linguistic value involves one of the {\em
primary terms} (e.g. tall and short), whose meaning are
subjective and context dependent.
\paragraph{} Furthermore, linguistic values of a linguistic
variables are generated from primary terms using {\em linguistic
hedges}. Formally linguistic hedges are unary function that
modify the meaning of their argument in a specified,
context-independent way.
\paragraph{} This concept of linguistic variables provides a
basis for approximate reasoning - a reasoning that is apparently
qualitative and is neither too exact nor too inexact, which has
been proven to be an effective approach to model human's
reasoning. However as we noticed, linguistic variables are still
numerical in nature, and hedges are also just numerical
functions. As we shall see, using numerical values to model
human's linguistic reasoning comes with its own limitations as
well, and hedge algebra is a viable approach to address these
limitations.
\subsection{Hedge Algebra: an approach to domains of linguistic
variables}
\paragraph{} As mentioned above, Zadeh's approach to linguistic
variables and linguistic hedges has its own shortcomings:
\begin{itemize}
        \item It is easy to observe that one can compare
                lintuistic terms based on their intuitive meaning. For
                example, it's clear that true $\ge$ false, or
                true $\ge$ approximately true, or true $\ge$
                very$^n$ approximately true, for any natural
                number n. Embedding linguistic values domain into
                [0..1] however does not respect this order, since
                hedges are just interpreted as functions on
                numerical values, while naturally a linguistic
                hedge can have different behavior when applies to
                different linguistic terms. For example, intuitively
                possibly true is just ``perhaps true'', but less
                possibly true weakens the ``perhaps'' part, which
                makes less possibly true $\ge$ possibly true. But
                less true is definitely less than true. Zadeh's
                hedges just simply failed to capture this
                intuitive ordering of linguistic values.
        \item With Zadeh's linguistic variables, we actually have
                no ways to directly manipulate linguistic terms,
                since what we have are just labels being assigned
                with numerical values.                                 
\end{itemize}
\paragraph{} Because linguistic terms in fact have a natural ordering, and we
need a way to directly manipulate linguistic terms as
mathematical objects in their own right, it would be much better
if we had an algebraic structure that can model linguistic terms
instead of embedding them into the interval [0..1]. To this end,
in 1990 Nguyen Cat Ho and Wolfgang Wechler$^{\text{citation
needed}}$ presented {\em Hedge algebra}, an algebraic structure
that provides exactly the aforementioned needs.
\paragraph{} With hedge algebra, not only we can now directly manipulate
linguistic values, but also automated reasoning can be performed
directly on linguistic values without having to map them into
numerical values. This is without a doubt a very powerful approach 
to computationally model the way human reason, and since hedge
algebra was first presented the researches on automated reasoning
using hedge algebra as truth domain has been substantial, notably
in$^{{\text citation and citation needed}}$.
\paragraph{}
Very recently a new and promising reasoning method on a specific
subclass of hedge algebra called Linear Symmetrical Hedge Algebra
(LSHA) - the $\alpha$-Resolution method, has been published.
Currently there is no known implementation of this reasoning
method, and as will be discussed in the next section, that will
be one of the motivation for this thesis.

\section{Objectives of the thesis}
\subsection{The problem} On the one hand, there are many extensive researches with
impressive results on automated reasoning with hedge algebra, but
on the other hand, there haven't been many actual automated
reasoning systems applying these results. For an interested but
otherwise unconvinced person, this lack of implementations may raise several
questions: 
\begin{itemize}
        \item Is implementing automated reasoning with hedge
                feasible, or is it actually very hard to do in
                practice and only sounds good on paper?
        \item Is it an effective approach for modeling
                human's reasoning, i.e can it really reason with
                linguistic terms like we humans do?
        \item Can we apply this to solve any practical problem,
                or is this just of purely theoretical interest?
\end{itemize}
\subsection{Objectives} This thesis seeks to change this situation, by
providing an implementation of an automated reasoning system on
fuzzy logic with LSHA as its truth domain. In particular,
the system will use the aforementioned $\alpha-Resolution$ to show that
this specific method is feasible and effective, and in turn to
show that automated reasoning with hedge algebra can be feasible
and effective in general. To make a point of
applicability, the system will be designed to use in
knowledge-based systems.
\subsection{Overview of the rest of the thesis}
Chapter 2 constitutes the foundation of this
whole thesis, including the theory and intuitions behind hedge
algebra, the propositional fuzzy logic with LSHA as its truth
domain, resolution and $\alpha-Resolution$ reasoning methods on
this logic.\\
Chapter 3 addresses the methodological approaches to achieve the
objectives of this thesis.\\
Chapter 4 deals with the design and implementation details of the
system.\\
Chapter 5 briefly summarizes the whole thesis, discusses its
limitations and directions for further researches.
\chapter{Theoretical Background}

\subfile{part1/sec2-1.tex}
\subfile{part1/sec2-2.tex}

\chapter{Methodological Approaches} 
\paragraph{} The first section is a brief description of the
programming paradigm of choice for this implementation. Although
brief, this is still the longest section of this chapter since
it has the most major influences in other design decisions.\\
The next two sections are about the overall architecture of the
program, which influent how to decomposed and arrange components
of the programs.\\
The last section is about the data modeling methodology of this
implementation. 
\section{Functional Programming paradigm}
%{\huge //TODO}
\paragraph {Functional programming}is a programming paradigm where
programs are executed by evaluating expressions, in contrast with
imperative programming where programs are composed of statements
which change global state when executed. Functional programming
typically avoids using mutable state. 
\paragraph {Functional programming is {\it{\bfseries pure}}}iff every function being used is a pure function, i.e
its returned value only depends on its parameters, and does not
depend on any mutable state. Functions in purely functional
programming are the same as mathematical functions, in fact the
denotational semantics of functions in a purely functional
programming language is simply its equivalent mathematical function.
\paragraph {Features of purely functional programming}including but not
necessarily limited to:
\begin{itemize}
       \item {\bfseries Declarative: } Functional programming is
               about specifying what the computation is, not how
               to compute the computation.
       \item {\bfseries First class and higher order functions: }
               Functions being first class means that functions
               are just another type of data: one can bind them
               to variables, pass them to other function,
               return them from other function, create them on
               the fly or use them to
               define new data type. Higher order functions are
               functions that take functions as parameters or
               return new function. The presence of higher order 
               functions implies first-class status of functions.
       \item {\bfseries Referential transparency: } When every
               expression is not dependent on state, one can
               subtitute an expression with its equivalent, or
               even reorder expressions and still preserve the
               program's semantics.
       \item {\bfseries Liberty of evaluation strategy/order: }
               Programs can be evaluated in any order without
               affecting its semantics, contrary to imperative
               programming where statements must be evaluated in
               order. 
       \item {\bfseries Expressive type system: } While type
               system is a no-brainer in any modern language,
               the most advanced state-of-the-art type systems
               are most usually developed along side with
               functional languages. 
       \item {\bfseries Automatic memory management: } Obsolete
               memory references are handled by the garbage
               collector. This helps eliminate a large class of
               hard to debug runtime errors. Function being first
               class actually requires automatic memory
               management: an inner function can refer to a
               variable of an outer function, and this inner
               function can be bind to a variable that can exist
               beyond the extent of the outer function. So the
               ``free'' reference in the inner function must also live
               beyond its defining function. Manual memory
               management is impossible within this scenary.
       \item {\bfseries Explicit side effects and seperation of
               pure and IO code: } A computer program after all
               still has to perform effectful computation to be
               useful. Purely functional languages provide side
               effects like IO without breaking purity by {\it
               lift} effectful computations into a different
               context, completely seperated from the
               ``workhorses'' pure functions that do (almost) all
               the computation, so that IO code can never change
               behaviors of pure code. This explicitness actually
               makes debugging both computing code and IO code much 
               easier than in imperative languages, since they
               are completely decoupled: a function that
               generates correct results given a specific input
               will always do so, and input/output values are fine as
               long as they are right: there's no way that after reading
               from a file for example a function would return different 
               result than it did before.
\end{itemize}                
\paragraph {}Benefits of those features including:
\begin{itemize}
        \item {Declarativeness} makes program more abstract and actually
          resembles mathematical formulae. This is a big plus for
          project like this one, where the main task is to
          implement mathematical constructs    
        \item {Higher order functions} makes
                programs more modular and elevates code reuse,
                since higher order functions can easily capture
                recurring patterns of computation. Programming
                with higher order function leads to much more
                succint programs.
        \item {Referential transparency} enables many kinds of
                otherwise impossible automatic
                transformation/optimization of programs. This is
                more of an interest for compiler implementors,
                but referential transparency also enables 
                {\it equational reasoning}. By
                subtituting expressions without breaking the
                program's semantics, one can perform reasoning
                about programs much like about other kinds of
                algebraic expressions. This makes proving
                program's properties actually viable. Sometimes
                one can even use equational reasoning to derive
                valid program from specification.\\
                Referential transparency also helps with a
                program's reliability: once a function is
                demonstrated to be correct, it is assured to be correct and
                will never do anything unexpected because of some
                unforeseeable global change of state.                
        \item Liberty of evaluation strategy/order's benefits are
                twofold. On the one hand, that means
                parallelization is much more feasible with
                functional programs, since expressions can be
                in any orders, including {\it many at the same
                times}. On the other hand (and this is the more
                relevant benefits for this project), it enables
                the use of lazy evaluation: value will only be
                evaluated as needed, and evaluated value will
                be cached for later use. While this can increase
                perfomance, the more important application is {\it
                infinite data structures} - this helps
                programmers have more freedom and can increase
                expressiveness a whole lot.              
       \item An expressive type system provides various kinds of
               polymorphism, catches various errors at compile
               time more effectively than other conventional type
               system, and makes modeling mathematical structures
               natural.\\
               The last point is worth emphasized for its
               direct relevance to this thesis. Functional type
               system usually provides {\it algebraic data type}
               as a basic type defining method. It can be thought
               of as a unification of record type, union type and
               parametric type: a data type can have many forms,
               each form can have many fields, each field can
               recursively have the same type as the defined
               type, and the types of fields can be {\it type
               variables}, which means a type can be
               parametrically polymorphic. In addition with a way
               to define (overload) common operations amongs
               different types, e.g Haskell's {\it type class}, this
               leads to a natural way of expressing mathematical
               structures in programs.  For example, as we know
               the set of natural numbers and the set of real
               numbers can both form a group over identity 0 and
               operation (+). To express this in for example
               Haskell, one would define a Group type class that
               requires definition of the identity and the
               operation. Then natural numbers, real numbers or
               any other type possibly be a group in a
               mathematical sense can be a Group, just by
               providing the two requirements.
       \item The last two mentioned features, as discussed,
               simply help programs less errorneous in general.        
\end{itemize}                
%\subsection{}
%\subsection{}
\section{Software architecture}
\subsection{Self-contained application} The system is self-contained,
not dependent on the services of any other software component (save
for the operating systems), and does not provide services for any
other software component. 
\subsection{Database-centric architecture} The system is data-centric
in nature, makes use of a database management system (an embedded DBMS
instead of a DBMS server in this case, to make the application
self-contained).
\subsection{Rule-based system} To be more precise, this system is a
platform for both creating knowledgebase and reasoning using the knowledgebase.

\section{Data Modeling Methodology}
\paragraph{Top-down data modeling: } Top-down data modeling methodology enforces that the
design decision of data model is made before the actual
implementation, and the implementation must proceed accordingly. For a
reasoning system like this, its entities and their relationships can
be made quite obvious with just a little upfront thinking, so it is
very appropriate to use top-down methodology.
\end{document}
