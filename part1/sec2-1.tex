\documentclass[part1.tex]{subfiles}
\begin{document}

\section{Hedge algebra}
\subsection{Intuitive properties of linguistic hedges}
\paragraph{} As discussed in the {\bfseries Introduction}
section, a linguistic hedge is an unary function that modifies
the meaning of their argument in a specified, context-dependent
way. However, this formal definition doesn't shed much insight on
the concept of linguistic hedges.
\paragraph{} Linguistic hedges in hedge algebra are originated
from natural language's hedges, so before investigate further on
hedge algebra, this section will first discuss about the
intuitive properties behind linguistic hedges, which are the
foundation to the formalization of hedge algebra
\paragraph{} First, let's consider the words Very, More, Less
and Possibly. Intuitively, when we use Very or More
as a modifier for another word, e.g. Nice, those words have an
strengthening effect on the ``argument'', for instance Very Nice
definitely has more ``niceness'' than Nice. On the other hands,
words like Approximately, Possibly and Less have a weakening
effect on the modified word. Possibly Old is kind of lacking in
``oldness'' than just plain Old. These modifier words are what we
call linguistic hedges.
\paragraph{} Also note that many linguistic variables have some
primary vague concepts, for instance the linguistic variable
Height has ``Tall'' and ``Short'', the variable Health has
``Strong'' and ``Weak'', and the variable Truth has ``True'' and
``False''. We usually think of a primary vague concepts of a
variable as having ``positive meaning'', like Strong or True, and the other
as having ``negative meaning''.
\paragraph{} Intuitively, linguistic hedges seem to have the following properties:
\begin{enumerate}
  \item Every hedge either strengthens or weakens the meaning of
    primary vague concepts, both positive and negative ones. We
    call the strengthening ones positive hedge, and the weakining
    ones negative hedge.
  \item A hedge can also strengthen or weaken the meaning of each
    others and itself. We can see that Very strengthens Very,
    More, Less but weakens the meaning of Possibly. If a hedge h
    strengthens the meaning of a hedge k, we say that h is
    positive w.r.t. k. If h weakens the meaning of k, we say that
    h is negative w.r.t. k.
  \item Also observe that different hedges can strengthen or
    weaken differently. It's clear that Very Dangerous is more of
    a threat than More Dangerous, for example.
  \item And lastly, while hedges can modify a vague concept, it
    can't inherently change the intuitive meaning of that
    concept, rather the modified meaning stems from the inherent
    meaning of the vague concept, and it has something to say about
    the meaning of the concept it stems from. Possibly True is
    saying something about True but not about False, and Very
    Handsome is not saying anything about Ugly, but rather about
    Handsome.
\paragraph{} The formalization of hedge algebra, as investigated
below, is intuitively rooted in those observations.
\end{enumerate}
\subsection{Definition}

{\bfseries Hedge algebra:} Let \(X\) be a linguistic variable and its domain is \(X = Dom(X)\). A hedge 
algebra \(AX\) in respect to \(X\) is a 4-tuple \(AX = (X,G,H,\le)\) where G is the set of spanning
elements, H is the set of hedges and \(\le\) is the sematic ordering relation on X.
The set G usually has positive, negative and neutral spanning elements. In practice, G usually has only
positive and negative spanning elements e.g \{True,False\}, \{High, Low\}.\\

A hedge algebra AX satisfies the following  axioms:
\begin{enumerate}

        \item Hedges either increase or decrease the effect of other hedges including itself, and it is 
then positive or negative w.r.t. the other.

        \item If \(u\notin H(v)\) and \(v\notin H(u)\) then \(\forall x\in H(u), x\notin H(v)\) and vice versa. 
	Furthermore if u and v cannot be compared then so are x,y for all \(x\in H(u), y\in H(v)\).


        \item \(x \not = hx, x \notin H(hx). h\neq k, hx<kx\) then \(h'hx\le k'kx\forall h,h',k,k'\in
	H.\) Furthermore, if \(hx \neq kx\) then hx and kx are independent w.r.t. to each other.


        \item \(u \in v, u \le v\implies u\le hv\) for \(\forall h\) and vice versa.
\end{enumerate}
\paragraph{} Those axioms mean that:
\begin{enumerate}
        \item The first axiom is self-explainatory.
        \item The second axiom means that if two vague concepts are really different, then they form seperated concept categories, which means they don't have any common meaning.
        \item The third one means that each hedge has its own meaning and defines its own concept category.
        \item The last one means that a hedge only modifies the vague concept's meaning. It preserves the ordering relation of the vague concept's meaning w.r.t. other vague concepts.
\end{enumerate}
\subsection{Properties}
\paragraph {Semantic heredity:} When a hedge affects the meaning of a linguistic value, it only
	increases or decreases a little bit the meaning of that value. The resulting value inherits
	most of its parent's meaning i.e the parent's comparability is preserved: 
	if hx \(\le\) kx then H(hx) \(\le\) H(kx). Generally, \(\forall (u,v \in X, 
	\sigma_1 = h_n..h_1, \sigma_2 = k_m..k_1, h_i,k_j \in H)\), we have:
	\(u \le v \implies \sigma_1u \le \sigma_2v\)\\
{\bfseries Theorem 1:} Let AX = (X,G,H,\(\le\)). The following statements hold:\\
\indent (1) If x \(\in\) X is a fixed point of an operation h in H, i.e. hx = x, then it is a fixed
	 point of the others.\\
\indent (2) If x = \(h_n \cdots h_1 u\), and there exists an index \(i<n\) such that \(h_i \cdots h_1 u\)        is another representation (canonical) of x w.r.t. u and \(h_j x = x, \forall j\ge i\).\\
\indent (3) For any \(h, k\in H\), if \(hx\le kx\) and \(h\neq k\) then \(hx< kx\).\\
\indent \(\implies\) The canonical representation of any linguistic value w.r.t. another value is unique.	\\
{\bfseries Theorem 2:} Let \(x = h_n \cdots h_1 u\) and \(y = k_m \cdots k_1 u\) be two arbitrary
	canonical representations of x and y w.r.t. u, respectively. Then there exists an index
        \(j \le min(n, m) + 1\) such that \(h_i = k_i, \forall i < j\), and\\
\indent (1) \(x < y \iff h_j x_j < k_j x_j ,\: where \: x_j = h_j-1 \cdots h_1 u\)\\
\indent (2) \(x = y \iff n = m = j\: and\: h_j x_j = k_j x_j\)\\
\indent (3) x and y are incomparable \(\iff h_j x_j\) and \(k_j x_j\) are incomparable\\
\paragraph {Sign function:} The function $Sign: X \rightarrow
\{-1,0,1\}$ is a mapping defined recursively as follows, where
hedges h and h' are arbitrary and $c \in c^{-}, c^{+}$
\begin{enumerate}
        \item $Sign(c^{-}) = -1, Sign(c^{+}) = 1$,
        \item $Sign(h'hx) = -Sign(hx)$ if $h'hx \neq hx$ and h'
                is negative w.r.t. h (or w.r.t. c, if h = I and x
                = c)
        \item $Sign(h'hx) = Sign(hx)$ if $h'hx \neq hx$ and h'
                is positive w.r.t. h (or w.r.t. c, if h = I and x
                = c)
        \item $Sign(h'hx) = 0$ if $h'hx = hx$
\end{enumerate}
{\bfseries Theorem 3:} For any h and x, if $Sign(hx) = 1$ then
$hx > x$, and if $Sign(hx) = -1$ then $hx < x$
\indent \(\implies\) Theorem 2 and 3 provide us the mean to
compare two linguistic values in a hedge 
algebra.
\paragraph{Example:} Assuming we have $AX = \{X, G =
\{False, True\}, H=\{V,L\}, \le\}$, V is positive w.r.t. to
False, True, L and itself; L is negative w.r.t. to
False, True, V and itself. Then:
\begin{enumerate}
        \item L True $\le$ True $\le$ V True
        \item V L True $\le$ L L True, because V is positive
                w.r.t. L, and L is negative w.r.t. True, which
                makes $Sign(V L True) = Sign(L True) = -1$, which
                means V L True $\le$ L True, while $Sign(L L
                True) = -Sign(L True) = 1$, which means L L True
                $\ge$ L True. Thus, V L True $\le$ L L True
        \item Because V L True $\le$ L L True, we also have $\sigma1$ 
                V L True $\le$ $\sigma2$ L L True, where
                $\sigma1$ and $\sigma2$ are arbitrary hedges
                string. 
\end{enumerate}


\subsection{Linear symmetrical algebra}
The spanning set G usually has two comparable linguistic values. For example, we have False \(<\) True
for truth value. A hedge algebra that has only two primitive linguistic values is called a symmetrical
hedge algebra. The spanning set is denoted \(G = \{c^-, c^+\}, c^+, c^-\) are positive and negative 
spanning element, respectively.\\\\

The set H of hedges can also be partitioned into two seperated subsets \(H^+ = \{h|hc^+ > c^+\} = 
\{h|hc^- < c^-\}, H^- = \{h|hc^- > c^-\} = 
\{h|hc^+ < c^+\}\). Any two hedges in \(H^+\) (or \(H^-\)) are comparable to each other, and each hedge in \(H^+\) is converse to a hedge in \(H^-\) and vice versa. A hedge algebra \(AX = (X,\{c^+,c^-\},H^+\cup
H^-, \le)\) is linear symmetrical iff \(H^+\) and \(H^-\) are linearly ordered. It is easy to see
that X is also linearly ordered by \(\le\).\\\\

Let I \(\notin\) H be the {\em identity hedge}: \(\forall x \in X, Ix = x\). I is smaller than any other
hedge in \(H^+\) and \(H^-\).\\\\

If \(H \neq {\o}, H(c^+), H(c^-)\) are infinite then \(inf(c^+) = sup(c^-) = W, inf(c^-) = 0, sup(c^+) = 1\). W is call the neutral element. We have: \(0<c^-<W<c^+<1\). Denote the linguistic value domain as
\(\bar X = X \cup \{0,W,1\}\)\\\\

From that we define:
\begin{itemize}
\item Let \(x = \sigma c, \sigma \in H^*, c \in \{c^+, c^-\}. y\: \text{is the negation of x, denoted}
	y = -x\: \text{ if }\: y = \sigma c', \{c,c'\} = \{c^+, c^-\}. -0 = 1, -1 = 0, -W = W\).
\item \(x,y \in \bar X\) then \(x \wedge y = min(x,y), x \vee y = max(x,y)\)
\end{itemize}


\subsection{Rule of moving hedges}
\indent (RT1): \(\frac{((P,hu),\sigma <True|False>)} {((P,u),\sigma h<True|False>)}\)
\\
\\
	(RT2): \(\frac{((P,u),\sigma h<True|False>)} {((P,hu),\sigma <True|False>)}\)
\paragraph{} For example, considering we have the linguistic
variable Height with value [Short..Tall], assumming `` `Jordan is
{\em Very Very} {\bf Tall}' is {\bf True}'', we can equivalently
say that `` `Jordan is {\bf Tall}' is {\em Very Very} {\bf
True}''. 
\paragraph{} The Rule of moving hedges described above gives us a
way to {\em move} the meaning of arbitrary linguistic value to
the domain of linguistic truth value. With this, any kind of
linguistic reasoning can be grounded to reasoning on linguistic
fuzzy logic with linguistic truth values as its truth domain.
\section{Fuzzy propositional logic with truth value domain based on
LSHA}
\paragraph{} In this section, we investigate the syntax and
semantics of fuzzy propositional logic with LSHA as its truth
domain. 
\paragraph{} {\em Syntax} is a collection of rules used for
constructing, or transforming the symbols and words of a
language. In constrast, {\em Semantics} is the meaning of that
language. Syntax and semantics are the most fundamental
components of any formal language and logic.
\subsection{Syntax}
\paragraph{} An {\bfseries alphabet} of a logic is the set of
all symbols that can be used to construct a syntactically valid
proposition.
\paragraph{} The alphabet of this logic consists
ofthe following classes of symbol:\\
\indent + Atom symbols: A,B,C \ldots\\
\indent + Linguistic value symbols: a,b,c \ldots\\
\indent + Constant symbols: 0,1,W\ldots\\
\indent + Logical connectives:\(\vee,\wedge,\to,\neg\) \ldots\\
\indent + Auxiliary symbols: ',', '(', ')'\ldots\\ \\
\paragraph{} A {\bfseries literal} is an ``atomic'' unit that
has meaning in a logic. If alphabet symbols are letters, then literals
are words in the language of logic. For classical logics, for any
atom A, we have two corresponding literals for
it: either A or $\neg{\text{A}}$. We can consider it as if we
annotated the symbol with a truth value, in this case True for
plain A and False for $\neg{\text{A}}$. We can generalize this
observation to define a literal for multi-valued logic, such as
the logic being under consideration.\\
A string \(A^a\) is called a literal, where
\(A\) is an atom and \(a\) is a truth value in \(A\)'s domain.

\paragraph{} A {\bfseries formula} is a syntactically legal string of alphabet
symbols. It is like a meaningful, complete sentence in the language of
logic. A formula is recursively defined as follows:\\
 \indent + A literal is a formula\\
 \indent + F is a formula then \( (\neg F\)) is also a formula\\
 \indent + F and G are formulae then \( (F\vee G), (F\wedge G), (F \to G)\) are also formulae\\
 \indent + Only strings generated by the above rules are formulae\\
 \indent ( {\bfseries Precedence of operations:} \(\neg\)  >>  \(\to\)  >>  \(\wedge\)  >>  \(\vee\))\\

\subsection{Semantic}
\paragraph{} The previous subsection introduced the syntax of the
language of fuzzy propositional logic based on LSHA. The meaningful
unit of construct in this language is a formula, which is like a
declarative statement in natural language. Such sentences refer
to some ``possible world'' and may have different truth value in
``this world''. In classical logics, the meaning of a logic 
formula is defined relative to some abstract world by assigning
the elements of the language to some abstract structure. And for
this logic, the structure being used is the linguistic truth value 
in Linear Symmetrical Hedge Algebra.
\subsubsection{Interpretation}
\paragraph{} An {\bfseries Interpretation} I:\{A = a1\} of the literal
\(A^{a2}\) is the mapping associate the formula
with a value of the truth value domain. In other words,
interpretation is the assignment of truth value to atom
symbols, from which the truth value of literals and formulae can
be determined.\\
To put it simply, this assignment of truth value is analogous to
putting some logical propositions under consideration in a
possible world.
\paragraph{} For instance, considering the proposition ``All cats
hate dogs''$^{\text{Possibly True}}$. Normally in our world, most
cats do hate dogs, and this proposition sounds likely to be true.
However in some alternate reality where every single cat
loves dogs, this proposition doesn't seem to be right. That is to
say, the truth value of the proposition ``All cats hate dogs''$^{\text{Possibly True}}$
depends on the truth value of ``All cats hate dogs''.

\paragraph{} Let A be an atom, $I:\{A=a1\}$ be an interpretation.
The truth value of a literal $A^{a2}$ under the interpretation
$I:\{A = a1\}$ is defined as follows:\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = a1 \wedge a2 \) if a1, a2 \(\ge W\)\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = \neg (a1 \vee a2) \) if a1, a2 \(<\) W\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = \neg a1 \vee a2 \) if a1 \(\ge\) W, a2 \(<\) W\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) =  a1 \vee \neg a2 \) if a1 \(<\) W, a2 \(\ge\) W\\

\paragraph{Logical connectives' semantics:} let a and b be some linguistic value. Then:\\
\indent + \(a \vee b = max(a,b)\)\\
\indent + \(a \wedge b = min(a,b)\)\\
\indent + \(\neg a = \text{symmetrical value of } a\)\\
\indent + \(a \to b = max(\neg a,b)\)\\

Let T(A) be A's truth value under an arbitrary interpretation.\\
\paragraph{Truth value of formulae:} The truth value of a formula
under an interpretation T is defined as follow:\\
\indent + If A is a literal, T(A) is determined by the interpretation\\
\indent + Let A and B be two formulae:\\
\indent \indent (1) \(T(A\vee B) = T(A) \vee T(B)\)\\
\indent \indent (2) \(T(A\wedge B) = T(A) \wedge T(B)\)\\
\indent \indent (3) \(T(A\to B) = T(A) \to T(B)\)\\
\indent \indent (4) \(T(A\neg B) = T(A) \neg T(B)\)
\paragraph{Properties of conjunction and disjunction operators
under an interpretation:} let A,B,C be formulae, and I be an
arbitrary interpretation. Then the following properties hold:
\begin{itemize}
\item Commutativity:
  \begin{itemize}
    \item $I(A \vee B) \equiv I(B \vee A)$
    \item $I(A \wedge B) \equiv I(B \wedge A)$
  \end{itemize}
\item Associativity:
  \begin{itemize}
    \item $I(A \vee (B \vee C)) \equiv I((A \vee B) \vee C)$
    \item $I(A \wedge (B \wedge C)) \equiv I((A \wedge B) \wedge C)$
  \end{itemize}
\item Distributivity:
  \begin{itemize}
    \item $I(A \vee (B \wedge C)) \equiv I((A \vee B) \wedge (A \vee
      C))$
    \item $I(A \wedge (B \vee C)) \equiv I((A \wedge B) \vee (A \wedge
      C))$
  \end{itemize}
\end{itemize}
\subsubsection{Satisfiability, unsatisfiability, falsehood and
tautology}
\paragraph{} Like it was mentioned above, the act of assigning
truth value to atoms is analogous to putting a proposition under
a possible world. For a proposition, it most likely has different truth
value in different possible worlds. In some world, it may still
be considered something ``true'', but in some other world, it
may be something ``false''. Some propositions are always
``false'' in every world, while some others are always true.
This brings us to the concepts of
satisfiability, unsatisfiability, falsehood and tautology.\\\\
{\bfseries Definition:} Let S be a formula, I be an arbitrary interpretation:\\
\indent + S is false under I if \(T_I(S) < W\)\\
\indent + S is unsatisfiable if it is contradicted to every interpretation\\ 
\indent + S is satisfiable if \(\exists I: T_I(S) \ge W\). I is then called a model of S, denoted \(I \models S\)\\
\indent + S is a tautology if \(\forall I: T_I(S) \ge W\)\\
\indent \(\implies)\) Collorary: A formula A is a tautology iff \(\neg\)A is unsatisfiable.
\subsubsection{Logical equivalences. Functional complete set of logical connectives}
\paragraph{} Two formulas A and B are said to be equivalent (\(A \equiv B\)) if A and B have the same \\
indent truth value for every interpretation I.
\paragraph{} Let A, B and C be formulae, then the following
properties hold:
\begin{itemize}
\item Idempotency:
  \begin{itemize}
    \item $A \vee A \equiv A$
    \item $A \wedge A \equiv A$
  \end{itemize}
\item Implication:
  \begin{itemize}
   \item $A \rightarrow B \equiv \neg A \vee B$
   \item $A \leftrightarrow B \equiv (A \rightarrow B) \wedge (B
     \rightarrow A)$
  \end{itemize}
\item Double negation:
  \begin{itemize}
    \item $\neg\neg{A} \equiv A$
  \end{itemize}
\item De Morgan:
  \begin{itemize}
    \item $\neg{A \vee B} \equiv \neg{A} \wedge \neg{B}$
    \item $\neg{A \wedge B} \equiv \neg{A} \vee \neg{B}$
  \end{itemize}
\item Commutativity:
  \begin{itemize}
    \item $A \vee B \equiv B \vee A$
    \item $A \wedge B \equiv B \wedge A$
  \end{itemize}
\item Associativity:
  \begin{itemize}
    \item $A \vee (B \vee C) \equiv (A \vee B) \vee C$
    \item $A \wedge (B \wedge C) \equiv (A \wedge B) \wedge C$
  \end{itemize}
\item Distributivity:
  \begin{itemize}
    \item $A \vee (B \wedge C) \equiv (A \vee B) \wedge (A \vee
      C)$
    \item $A \wedge (B \vee C) \equiv (A \wedge B) \vee (A \wedge
      C)$
  \end{itemize}
\end{itemize}
\subsubsection{Conjunctive normal form}
\paragraph{} A formula expressed as a conjunction of formulae, where these formulae are in turn expressed as
a disjunction of literal, is said to be in {\em Conjunctive
normal form}, or CNF for short. CNF is important, because as we
shall see in the next section, a knowledge base must be represented in CNF to be able
to use resolution on it.
\paragraph{Theorem} Every formula in our propositional logic is logically equivalent to a CNF
formula.\\
The following algorithm can be applied to transform any
propositional sentence into its CNF:\\\\
\indent- Eliminate \(\to\) and \(\leftrightarrow\) :
\begin{align*}
	&F \to G \underset{CNF}{\implies} \neg F \vee G\\
	&F \leftrightarrow G \underset{CNF}{\implies} (\neg F \vee G) \wedge (\neg G \vee F)
\end{align*}
\indent- Move \(\neg\) inward: 
\begin{align*}
	&\neg (F \vee G) \underset{CNF}{\implies} \neg F \wedge \neg G
\end{align*}
\indent- Elimitate double negation:
\begin{align*}
	&\neg \neg F \underset{CNF}{\implies} F
\end{align*}
\indent- Applying distributivity law:
\begin{align*}
	&F \wedge (G \vee H) \underset{CNF}{\implies} (F \wedge G) \vee (F \wedge H)\\
	&F \vee (G \wedge H) \underset{CNF}{\implies} (F \vee G) \wedge (F \vee H)
\end{align*}	
\indent- Rewrite redundant formula:
\begin{align*}	
	&F \vee F \underset{CNF}{\implies} F\\
	&F \wedge F \underset{CNF}{\implies} F
\end{align*}
\end{document}
